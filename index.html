<!DOCTYPE html>
<html lang="en">

  <head>
    <title>
      UnImplicit: Understanding Implicit and Underspecified Language
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Prevent caching -->
    <META HTTP-EQUIV="Pragma" CONTENT="no-cache">
    <META HTTP-EQUIV="Expires" CONTENT="-1">
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet" media="screen">
    
    <style>
      /* Customize container */
      @media (min-width: 968px) {
	  .container {
	      max-width: 968px;
	  }
      }
      .container-narrow > hr {
	  margin: 30px 0;
      }
      
      /* Customize dropdown menu */
      .dropdown {
	  cursor: pointer;
      }
      .dropdown sup {
	  color: rgb(66, 139, 202);
      }
      .dropdown sup:hover, .dropdown sup:focus {
	  color: rgb(42, 100, 150);
	  text-decoration: underline;
      }
      .dropdown-menu {
	  min-width: 500px;
	  left: -200px;
      }
      .dropdown-menu li {
	  margin-bottom: .5em;
	  /*border-top-style:solid; padding-left:10px;*/
      }
    </style>
  </head>
  
  <body data-spy="scroll" data-target="#navbar" data-offset="70" onload="load()">
    
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">UnImplicit</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="#organizers">Organizers &amp; Committee</a></li>
		  <li><a href="#dates">Important Dates</a></li>
		  <li><a href="#speakers">Speakers</a></li>
		  <li><a href="#program">Program</a></li>
		  <li><a href="#submission">Submission Information</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <!--li><a href="https://www.emnlp-ijcnlp2019.org/">EMNLP-IJCNLP 2019</a></li-->
          </ul>
        </div>
      </div>
    </nav>
    
    <div class="container">
      
      <br/>
      <br/>      
      <br/>
      <center><h2 id="top"><b>UnImplicit: The Second Workshop on<br/>
	    Understanding Implicit and Underspecified Language</b></h2></center>
      <center><h4>at NAACL 2022, Seattle</h4></center>
      <br/>
      	        <img src="logo.png" alt="logo" style="float:left;width:300px;height:300px;">

      <p>Implicitness and underspecification are ubiquitous in language. Specifically, language utterances may contain <i>empty</i> or <i>fuzzy</i> elements, such as the following: units of measurement, as in <i>she is 30</i> vs. <i>it costs 30</i> (30 what?), bridges and other missing links, as in <i>she tried to enter the car, but the door was stuck</i> (the door of what?), implicit semantic roles, as in <i>I met her while driving (who was driving?)</i>, and various sorts of gradable phenomena; is a <i>small elephant</i> smaller than a <i>big bee</i>? Even though these phenomena may increase the chance of having misunderstandings, our conversational partners often manage to understand our utterances because they consider context-specific elements, such as time, culture, background knowledge and previous utterances in the conversation.</p>

        <p>In particular, (implicit) domain restrictions reveal the problem of context dependence of the interpretation of utterances (Stanley and Gendler Szab ́o, 2000). While certain expressions might not be underspecified per se, their interpretation is implicitly restricted by the broader discourse they appear in. This is especially valid for expressions with quantifiers such as <i>every marble is red</i>, which is only true for certain sets of marbles. The problem of underspecified language also extends to pragmatics: Presuppositions or implicatures are by definition not explicitly stated (Stalnaker et al.,1977; Beaver, 1997).  In addition, collaborative games datasets have revealed that speakers often employ underspecified language (Djalali et al., 2012).</p>

      <p>Despite the recent advancements on various semantic tasks, modeling implicitness and underspecification remains a challenging problem in NLP because the elements are not realized on the surface level.  Instead of relying on superficial patterns, models are required to leverage contextual aspects of discourse that go beyond the sentence-level.  Often, there is a lack of available resources to train such models due to the need for human annotation.  Multiple datasets and tasks targeting implicit phenomena have been proposed in recent years, including implicit semantic role labels and event arguments (Gerber and Chai, 2010; Ruppenhofer et al., 2010; Moor et al., 2013; Ebner et al., 2020; Cheng and Erk, 2018,2019), bridging and noun phrase linking (R ̈osiger et al., 2018; Elazar et al., 2021) and other empty elements (Elazar and Goldberg, 2019; McMahan and Stone, 2020). These endeavours were usually narrow in scope and it is still not clear how current models can appropriately model how linguistic meaning is shaped and influenced by context.</p>
      
      
      <p>The first workshop on Implicit and Underspecified Language (held at ACL2021) brought a diverse group of NLP practitioners  and  theoreticians  together  to  address  the  challenges  that  implicit  and  underspecified  language  poses  on NLP. More specifically, the workshop brought to light the wide range of phenomena that fall under that topic and how researchers from different perspectives tackle them.  In addition, the workshop proved that there is a strong interest in implicit and underspecified language within the NLP community.  </p>

      <p>The goal of the second edition of the workshop is to continue eliciting future progress on implicit and underspecified language  with  a  strong  focus  on  the  annotation  and  development  of  aspects  that  go  beyond  the  sentence-level  of  the phenomena.  Similar to the first edition, we would accept theoretical and practical contributions (long, short and non-archival) on all aspects related to the computational modeling (benchmarks, evaluation schemes, annotation, applications)
 of phenomena such as: implicit arguments, fuzzy elements, zero anaphora, metonomy, and discourse markers. In addition,we specifically encourage papers with a strong focus on the interpretation of these elements on the discourse, pragmatic and cognitive levels of language understanding.</p>

	    
      <a name="speakers"></a>
      <div class="page-header">
	<h1>Invited Speakers</h1>
      </div>
      <div class="row">
	<div class="col-sm-6">
	  <div class="panel panel-default">
	    <div class="panel-heading">
	      <h2 class="panel-title"><a href="https://thegricean.github.io">Judith Degen</a></h2>Stanford University
	    </div>
	    <div class="panel-body">
	      <b>How can neural language models be leveraged for pragmatic theory-building?</b><br/>
		    		<img src="judith-degen.jpg" alt="logo" style="float:left;width:100px;height:100px;" hspace="10" vspace="10">
		    Utterances are notoriously underspecified with respect to the speaker's intended meaning. Linguistic pragmatics has long been devoted to characterizing the principles underlying listeners' contextual reasoning about intended meanings. The advent of Bayesian computational modeling has brought about a qualitative shift in our understanding of pragmatic reasoning by virtue of allowing for precise formalization of these principles. However, Bayesian models suffer from scaling and intractability issues. In this talk, I propose that NLP and pragmatic theory can mutually inform each other. I review a key set of phenomena for neural language models to capture, and lay out a path for neural language models to be leveraged in pragmatic theory-building.
	    </div>
	  </div>
	</div>
	<div class="col-sm-6">
	  <div class="panel panel-default">
	    <div class="panel-heading">
	      <h2 class="panel-title"><a href="https://people.cs.georgetown.edu/nschneid/">Nathan Schneider</a></h2>Georgetown University
		  </div>
	    <div class="panel-body">
	      <b>A pandemic’s worth of plastic utensils: a Spragmatic view on meaning</b><br/>
       <img src="nathan-schneider.jpg" alt="logo" style="float:left;width:100px;height:100px;" hspace="10" vspace="10">
       How do we think about meaning? It is easy to look at words that are written down and conclude that those are the source of most textual meaning, and any meaning that goes beyond compositional semantics is the exception. But another view is that this is precisely backwards—our understanding of language is dominated by extrasemantic machinery, with the explicit linguistic cues as scaffolding. My remarks will explore the consequences of this "Spragmatic" perspective for meaning representation and models of meaning.
		  </div>
	  </div>
	</div>
		<div class="col-sm-6">
	  <div class="panel panel-default">
	    <div class="panel-heading">
	      <h2 class="panel-title"><a href="https://michael-franke.github.io/heimseite/">Michael Franke</a></h2>University of Tübingen
		  </div>
	    <div class="panel-body">
	      <b>Helpfulness of answers and goal-signaling questions</b><br/>
		    		    	    <img src="michael-franke.jpg" alt="logo" style="float:left;width:100px;height:100px;" hspace="10" vspace="10">

              Relevance is a central notion in the study of communication, but it is multifaceted, if not elusive and therefore hard to approach formally. This talk tries to get closer to a formalization of useful notions of relevance by looking at experimental data based on which different information-theoretic concepts of relevance of an answer can be compared. It then introduces a probabilistic model of question and answer choice which pivots around an action-based notion of relevance of information and which predicts how helpfulness of an answer is informed by the goal-signaling quality of relevant questions.
	    </div>
	  </div>
	</div>
      </div>
	 
	    
      <a name="program"></a>
      <div class="page-header">
	<h1>Workshop Program</h1><br/>
	(all times shown in Pacific Time)
      </div>
      <p>
      Judith Degen's talk is unfortunately cancelled due to unforseen circumstances. We've moved breakout session II to an earlier time (see below).
      </p>
<p>
  <table class="table table-striped">
    <tbody>
      <tr>
	<td>08:30</td>
	<td>Opening</td>
      </tr>
      <tr>
	<td class="success">8:45</td>
	<td>
	  <b>Invited talk (Room: 701 Clallum, also streamed on zoom)</b><br/>
	  <!-- title <br/-->
	  <i>Michael Franke</i></td>
      </tr>
      <tr>
	<td class="success">09:30</td>
	<td><b>Virtual Poster Session (Gather.town)</b> <!--small class="text-muted">(Gather.town)</small--></td></td>
      </tr>
      <tr>
	<td class="success"></td><td>Pre-trained Language Models' Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context<br/>
	  <i>Yan Cong</i></td></tr>
      <tr>
	<td class="success"></td><td>Pragmatic and Logical Inferences in NLI Systems: The Case of Conjunction Buttressing<br/>
	  <i>Paolo Pedinotti, Emmanuele Chersoni, Enrico Santus and Alessandro Lenci</i></td></tr>
      <tr>
	<td class="success"></td><td>"Devils are in the Details'' — Annotating Specificity of Clinical Advice from Medical Literature<br/>
	  <i>Yingya Li and Bei Yu</i></td></tr>
      <tr>
	<td class="success"></td><td>Searching for PETs: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms<br/>
	  <i>Patrick Lee, Martha Gavidia, Anna Feldman and Jing Peng</i></td></tr>      
      <tr>
	<td class="info"><b>Abstracts</b></td><td>Generating Discourse Connectives with Pre-trained Language Models: Discourse Relations Help Yet Again<br/>
          <i>Symon Stevens-Guille, Aleksandre Maskharashvili, Xintong Li and Michael White</i></td></tr>
      <tr>
	<td class="info"></td><td>Looking Beyond Syntax: Detecting Implicit Semantic Arguments<br/>
	  <i>Paul Roit, Valentina Pyatkin, Yoav Goldberg and Ido Dagan</i></td></tr>
      <tr>
	<td class="info"></td><td>ULN: Towards Underspecified Vision-and-Language Navigation<br/>
	  <i>Weixi Feng, Tsu-Jui Fu, Yujie Lu and William Yang Wang</i></td></tr>
      <tr>
	<td class="info"></td><td>Bridging the Gap: Recovering Elided VPs in Coordination Structures<br/>
	  <i>Royi Rassin, Yoav Goldberg and Reut Tsarfaty</i></td></tr>
      <tr>
	<td class="info"></td><td>Inferring Implicit Relations with Language Models for Question Answering<br/>
	  <i>Uri Katz, Mor Geva and Jonathan Berant</i></td></tr>
      <tr>
	<td class="info"></td><td>Life after BERT: What do Other Muppets Understand about Language?<br/>
	  <i>Vladislav Lialin, Kevin Zhao, Namrata Shivagunde and Anna Rumshisky</i></td></tr>
      <tr>
	<td class="success">10:30</td>
	<td><b>Oral Session (Room: 701 Clallum, also streamed on zoom)
	      <tr>	
      <tr>
	<td class="success"></td><td>Pre-trained Language Models' Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context<br/>
	  <i>Yan Cong</i></td></tr>
      <tr>
	<td class="success"></td><td>Searching for PETs: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms<br/>
	  <i>Patrick Lee, Martha Gavidia, Anna Feldman and Jing Peng</i></td></tr> 
      </td></tr>
      <tr>
	<td class="success">11:00</td>
	<td><b>In-Person Poster Session (Regency Ballroom, 7th floor)</b></td>
      </tr>
      
<tr>
	<td class="success">13:30</td>
	<td><b>Breakout Session I (discussion / presentation)(Room: 701 Clallum or zoom)</b>
		<i>What is the range of implicit phenomena? And how should we use ML-based modeling of these phenomena and tasks?</i>
      </td></tr>
      <tr>
      <tr>
	<td class="success">14:15</td>
	<td>
	  <b>Invited talk (Room: 701 Clallum, also streamed on zoom)</b> <br/>
	  <i>Nathan Schneider</i>
	</td>
      </tr>
      <!--
      <tr>
	<td class="success">15:30</td>
	<td>
	  <b>Invited talk (Room: 701 Clallum, also streamed on zoom)</b> <br/>
	  <i>Judith Degen</i>
	</td>
      </tr>
      -->
      <tr>
	<td class="success">15:30</td>
	<td><b>Breakout Session II (discussion / presentation) (Room: 701 Clallum or zoom)</b>
		 <i>What are the next steps in implicit and underspecified language research?</i>
      </td></tr>
      <tr>
	<td>16:15</td>
	<td>(Official) closing (Room: 701 Clallum, also streamed on zoom)</td>
      </tr>
    </tbody>
  </table>
</p>



	    	    
	    <div class="page-header">
      <a name="dates"></a>
      <div class="page-header">
	<h1>Important Dates</h1>
      </div>
      <ul>
	<li>February 11, 2022: First Call for Workshop Papers</li>
	<li>March 11, 2022: Second Call for Workshop Papers</li>
	<li>DEADLINE EXTENDED to April 14, 2022: Workshop Paper or Extended Abstract Due Date for Softconf submissions</li>
	<li>April 21, 2022: Workshop Paper Due Date for papers with ACL Rolling Review reviews</li>
	<li>May 6, 2022: Notification of Acceptance</li>
	<li>May 20, 2022: Camera-ready papers due</li>
	<li>July 15, 2022: Workshop Date</li>
      </ul>

	    
	    <div class="page-header">
      <a name="submission"></a>
      <div class="page-header">
	<h1>Submission</h1>
      </div>
We invite two types of submissions:
<ul>
	<li>Archival: long (up to 8 pages) or short (up to 4 pages) papers, with unlimited additional pages for references. These papers should report on complete, original and unpublished research and cannot be under submission elsewhere. If accepted, archival papers will appear in the workshop proceedings.
<li>Non-archival: Extended abstracts, which can take two forms: 
<ul>
	<li>Works in progress, that are not yet mature enough for a full submission. Up to 2 pages, with unlimited pages for references.
<li>Already published work, or work currently under submission elsewhere, which can be submitted as a copy of the submission/publication (please indicate the venue where it has been submitted to).
	    </ul>
	</ul>
	All deadlines are 11:59PM UTC-12:00 ("anywhere on Earth").
     <p>Please submit your papers at <a href="https://www.softconf.com/naacl2022/UnImplicit2022/">https://www.softconf.com/naacl2022/UnImplicit2022/</a></p>
         <p>At this link you will find two submission options: one for papers having been reviewed at ARR and the other one for direct submissions.</p>
                  <p>Please use the <a target="_blank" href="https://github.com/acl-org/acl-style-files">ACL style templates</a>.</p>

	    <p>If you have any questions please email us at unimplicitworkshop -AT- gmail.com</p>

	    <div class="page-header">
      <a name="organizers"></a>
      <div class="page-header">
	<h1>Organizers</h1>
      </div>
        	<h2>Organizers</h2>
      <ul class="list-unstyled">
	<li><a target="_blank" href="https://valentinapy.github.io">Valentina Pyatkin</a>, Bar-Ilan University</li>
	<li><a target="_blank" href="https://www.ims.uni-stuttgart.de/en/institute/team/Anthonio/">Talita Anthonio</a>, Stuttgart University</li>
	<li><a target="_blank" href="https://dpfried.github.io">Daniel Fried</a>, FAIR and University of Washington</li>
      </ul>
        <h2>Advisory Committee</h2>
      <ul class="list-unstyled">
	<li><a target="_blank" href="https://www.ims.uni-stuttgart.de/en/institute/team/Roth-00006/">Michael Roth</a>, Stuttgart University</li>
	<li><a target="_blank" href="https://research.biu.ac.il/researcher/prof-reut-tsarfaty/">Reut Tsarfaty</a>, Bar-Ilan University</li>
	<li><a target="_blank" href="https://www.cs.bgu.ac.il/~yoavg/uni/">Yoav Goldberg</a>, Bar-Ilan University and AI2</li>
      </ul>

      <h3>Program Committee<a name="committee" ></a></h3>

      <ul class="list-unstyled">
<li>Maria Becker</li>
<li>Eunsol Choi</li>
<li>Vera Demberg</li>
<li>Yanai Elazar</li>
<li>Katrin Erk</li>
<li>Dan Goldwasser</li>
<li>Daniel Hershcovich</li>
<li>Jennifer Hu</li>
<li>Lucy Li</li>
<li>Philippe Muller</li>
<li>Aida Nematzadeh</li>
<li>Sebastian Pado</li>
<li>Roma Patel</li>
<li>Massimo Poesio</li>
<li>Chris Potts</li>
<li>Vered Shwartz</li>
<li>Elias Stengel-Eskin</li>
<li>Elior Sulem</li>
<li>Tiago Timponi Torrent</li>
<li>Nicholas Tomlin</li>
<li>Sara Tonelli</li>
<li>Luke Zettlemoyer</li>
      </ul>

      <br/>
      <br/>
      <br/>
      
    </div>    
  </body>
</html>
